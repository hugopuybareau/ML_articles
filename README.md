# Machine Learning Papers Notes

This repository contains my personal notes and summaries from reading various machine learning papers. It serves as a collection of insights, concepts, and key takeaways from foundational and contemporary research in the field.

## Papers Included

- `**Attention is All You Need**`: A deep dive into the Transformer model, which introduces a novel attention mechanism to replace traditional recurrent neural networks, enabling better parallelization and faster training.
- `**Efficient Estimation of Word Representation in Vector Space**`: Notes on word2vec, including the Continuous Bag of Words (CBOW) and Skip-Gram models, and the techniques used to efficiently learn word embeddings, such as negative sampling.

These notes are intended primarily for my own reference, but feel free to explore if you're interested in similar readings or perspectives on these topics.
